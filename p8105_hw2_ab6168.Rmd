---
title: "p8105_hw2_ab6168"
output: github_document
date: "2025-09-24"
---
Angelica Bailey


```{r setup, echo=FALSE, message = FALSE}
library(tidyverse)
library(readxl)
```


## Problem 1

Cleaning pols_month dataframe
```{r}
pols_month_df = 
  read_csv("data/pols-month.csv") |> 
  #breaking up date variable
  separate(mon,
    into = c("year", "month","day"), sep = "-") |> 
  #converting month name to number
  mutate(year = as.numeric(year),
    month = case_match(
      month,
      "01" ~ "jan",
      "02" ~ "feb",
      "03" ~ "mar",
      "04" ~ "apr",
      "05" ~ "may",
      "06" ~ "jun",
      "07" ~ "jul",
      "08" ~ "aug",
      "09" ~ "sep",
      "10" ~ "oct",
      "11" ~ "nov",
      "12" ~ "dec"),
    #creating new variable to take on dem (0) and gop (1).
    prez_gop = case_match(
      prez_gop,
      0 ~ "dem",
      1 ~ "gop"
    ),
    president = prez_dem) |> 
  select(-day,-prez_gop,-prez_dem)



#cleaning snp dataframe
snp_df = 
  read_csv("data/snp.csv") |> 
  separate(date,
    into = c("month", "day","year"), sep = "/") |> 
  #reformatting year
  mutate(year = as.numeric(year),
         year = case_when(
           year > 15 ~ year + 1900,
           year <= 15 ~ year + 2000
         ),
         month = case_match(
           month,
           "1" ~ "jan",
           "2" ~ "feb",
           "3" ~ "mar",
           "4" ~ "apr",
           "5" ~ "may",
           "6" ~ "jun",
           "7" ~ "jul",
           "8" ~ "aug",
           "9" ~ "sep",
           "10" ~ "oct",
           "11" ~ "nov",
           "12" ~ "dec")
        ) |> 
  select(-day) |> 
  relocate(year)



#cleaning unemployment dataframe
unemployment_df = 
  read_csv("data/unemployment.csv") |> 
  #switching from wide to long format
  pivot_longer(
    cols = Jan:Dec,
    names_to = "month",
    values_to = "per_unemp"
  ) |> 
  mutate(month = tolower(month), 
           year = Year) |> 
  select(-Year) |> 
  relocate(year)


```


**Joining datasets**

```{r}
#merging `pols` and `snp` datasets
pols_snp_df =
  left_join(pols_month_df, snp_df, by = c("year","month"))

#merging `unemployment` with result
fte_df =
  left_join(pols_snp_df, unemployment_df, by = c("year","month"))
```

**Description**

The `pols_month_df` dataset shows the number of national politicians who are democratic or republican at any given time. It contains the number of republican governors, senators, and representatives on the associated date as well as on the democratic side. The `snp` dataset shows the closing values of the Standard & Poor's stock market index on the associated date. Lastly, the `unemployment` dataset displays percentages of unemployment by month and year. After joining these datasets, the resulting dataset has 113,124 rows and 13 columns. The total range of years is from 1947 to 2015. The key variables of the dataset include `president`,`close`, `per_unmep`. 

\
\

## Problem 2

Reading and cleaning excel files
```{r}
#cleaning MrTrash file
mr_trash_df = 
  read_excel("data/trash-wheel.xlsx", sheet = "Mr. Trash Wheel") |> 
  #removing empty columns
  select(-...15,-...16) |> 
  janitor::clean_names() |> 
  rename(
    weight = "weight_tons",
    volume = "volume_cubic_yards") |> 
  filter(row_number() <= n() - 1) |>
  mutate(sports_balls = as.integer(round(sports_balls)),
         year = as.integer(year),
         wheel_name = "mr_trash")



#cleaning ProfessorTrash file
prof_trash_df = 
  read_excel("data/trash-wheel.xlsx", sheet = "Professor Trash Wheel") |>
  janitor::clean_names() |> 
  rename(
    weight = "weight_tons",
    volume = "volume_cubic_yards") |> 
  filter(row_number() <= n() - 1) |> 
  mutate(wheel_name = "professor_trash")



#cleaning GwynndaTrash file
gwyn_trash_df =
  read_excel("data/trash-wheel.xlsx", sheet = "Gwynns Falls Trash Wheel") |> 
  janitor::clean_names() |> 
  rename(
    weight = "weight_tons",
    volume = "volume_cubic_yards") |> 
  filter(row_number() <= n() - 1) |> 
  mutate(wheel_name = "gwynnda_trash")

```


Binding Datasets
```{r}
trash_wheel_df =
  bind_rows(mr_trash_df, prof_trash_df, gwyn_trash_df) |> 
  relocate(wheel_name)
```



Filtering Gywnnda Dataset To June 2022
```{r}
june2022_gwyn = filter(gwyn_trash_df, month == "June", year == 2022) |> 
  rename(cigar = "cigarette_butts")
```



**Description**

There are `r nrow(trash_wheel_df)` observations in the resulting dataset. The variables include `Among the variables, the total weight of trash collected by Professor Trash Wheel is `r sum(prof_trash_df |> select(weight))` tons. The total number of cigarette butts collected by Gwynnda in June of 2022 is  `r sum(june2022_gwyn |> select(cigar))`. 



\
\

## Problem 3


```{r}
#reading in csv files
zip_df = 
  read_csv("data/Zip Codes.csv") |> 
  janitor::clean_names()


zori_df =
  read_csv("data/zori_NYC.csv") |> 
  select(-StateName,-Metro,-RegionType) |> 
  #renaming variables
  rename(
    region_id = "RegionID",
    size_rank = "SizeRank",
    zip_code = "RegionName",
    state = "State",
    city = "City",
    county = "CountyName") |> 
  mutate(
    county = str_remove(county," County")
  ) |>
  pivot_longer(
    cols = "2015-01-31":"2024-08-31",
    names_to = "date",
    values_to = "rent_price"
  )
```


Creating Tidy Dataset

```{r}
#joining datasets
zori_zip_df = 
  left_join(zori_df, zip_df, by = c("zip_code", "county")) |> 
  select(-file_date,-state,-city,-state_fips) 
  
```

**Description**

From the resulting tidy dataset, there are a total of `r nrow(zori_zip_df)` observations. There are 320 unique zip codes and 42 unique neighborhoods. There are many missing values in the rental prices dataset. 


Finding Missing Zip-Codes
```{r}
missing_zips <- zip_df |> 
  anti_join(zori_df, by = "zip_code")
```

Some ZIP codes might not be shown in the Zillow dataset because there may be very few rental properties in those zip codes. There may not be enough data for Zillow to compute the observed rent index. For example, the 10041 zip code is a portion of the financial district which has mostly financial sites and landmarks. Another area is 10101 which includes businesses and amenities like restaurants and cafes. 


Comparing Rental Prices
```{r}
#filtering to January 2020 and 2021
jan_data <- zori_zip_df |> 
  filter(date == "2020-01-31" | date == "2021-01-31") |> 
  #removing missing prices
  drop_na(rent_price)

#pivot to wide format
jan_wide <- jan_data |> 
  pivot_wider(
    names_from = date,
    values_from = rent_price) |> 
  rename(jan_2020 = "2020-01-31", 
         jan_2021 = "2021-01-31") |> 
  drop_na(jan_2020) 

#creating variable showing difference between 2020 and 2021
jan_wide <- jan_wide |> 
  mutate(price_change = jan_2021 - jan_2020)


#arranging to top 10
top10 <- jan_wide |> 
  arrange(price_change) |> 
  slice(1:10) |> 
  select(county, neighborhood, zip_code, jan_2020, jan_2021, price_change)

#fixing missing neighborhood
top10$neighborhood[top10$zip_code == "10069" & is.na(top10$neighborhood)] <- "Upper West Side"


```


Creating Table

```{r}
knitr::kable(top10)
```


The top 10 neighborhoods that have the largest drop in price are all located in New York. The neighborhood with the biggest drop was Lower Manhattan (zip code: 10007). Demand for these rented properties may fallen due to Covid-19.



